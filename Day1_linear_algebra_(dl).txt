# -*- coding: utf-8 -*-
"""linear-algebra (DL).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bso96Qx9FF8QPVTW-07BpdxcWVbTwgMw

# Linear algebra
"""

!pip install mxnet
import mxnet
from mxnet import nd

"""In MXNet scalars are NDArrays with just one element. """

x = nd.array([3.0, 4.0])
y = nd.array([2.0, 1.0])

print('x + y = ', x + y)
print('x * y = ', x * y)
print('x / y = ', x / y)
print('x ** y = ', nd.power(x,y))

"""## Vectors

Vectors are e.g. ``[1.0,3.0,4.0,2.0]``. We use 1D NDArrays.
"""

x = nd.arange(5)
print('x = ', x)

x[3]

"""## Length, dimensionality and shape

The length of a vector is commonly called its $dimension$. As with an ordinary Python array, we can access the length of an NDArray
by calling Python's in-built ``len()`` function.

We can also access a vector's length via its `.shape` attribute.
The shape is a tuple that lists the dimensionality along each of its axes.
"""

x.shape

"""The word dimension is overloaded between number of axes and number of elements. **To avoid confusion, when we say *2D* array or *3D* array, we mean an array with 2 or 3 axes respectively. But if we say *$n$-dimensional* vector, we mean a vector of length $n$.**"""

a = 2
x = nd.array([1,2,3])
y = nd.array([10,20,30])
print(a * x)
print(a * x + y)

"""## Matrices

Just as vectors generalize scalars from order $0$ to order $1$,
matrices generalize vectors from $1D$ to $2D$.
Matrices, which we'll typically denote with capital letters ($A$, $B$, $C$),
are represented in code as arrays with 2 axes.
Visually, we can draw a matrix as a table,
where each entry $a_{ij}$ belongs to the $i$-th row and $j$-th column.


$$A=\begin{pmatrix}
 a_{11} & a_{12} & \cdots & a_{1m} \\
 a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \cdots & a_{nm} \\
\end{pmatrix}$$
"""

print(nd.arange(10))
A = nd.arange(20).reshape((2,10))
print(A)

"""We can access elements $a_{ij}$ by specifying row $i$ and column $j$. Leaving them blank selects via `:` takes all ements in the respective dimension. 

We can transpose the matrix through `T`. That is, if $B = A^T$, then $b_{ij} = a_{ji}$ for any $i$ and $j$.
"""

print(A.T)

"""## Tensors

Just as vectors generalize scalars, and matrices generalize vectors, we can increase the number of axes. When working with images the axes correspond to the height, width, and the three (RGB) color channels.
"""

X = nd.arange(24).reshape((2, 3, 4))
print('X.shape =', X.shape)
print('X =', X)

"""## Basic properties of tensor arithmetic

Given two tensors $X$ and $Y$ with the same shape,
$\alpha X + Y$ has the same shape
(numerical mathematicians call this the AXPY operation).
"""

a = 2
x = nd.ones(3)
y = nd.zeros(3)
print(x.shape)
print(y.shape)
print((a * x).shape)
print((a * x + y).shape)

"""## Sums and means

In math we express sums using the $\sum$ symbol.
To express the sum of the elements in a vector $\mathbf{u}$ of length $d$,
we can write $\sum_{i=1}^d u_i$. In code, we can just call ``nd.sum()``.
"""

print(x)
print(nd.sum(x))

"""We can similarly express sums over the elements of tensors of arbitrary shape. For example, the sum of the elements of an $m \times n$ matrix $A$ could be written $\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$."""

print(A)
print(nd.sum(A,0))

"""A related quantity is the *mean*. 
We calculate the mean by dividing the sum by the total number of elements. In code this is ``nd.mean()``.

$$\mathrm{mean}(\mathbf{u}) = \frac{1}{d} \sum_{i=1}^{d} u_i \text{ and }
\mathrm{mean}(A) = \frac{1}{n \cdot m} \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$$
"""

print(nd.mean(A))
print(nd.sum(A) / A.size)

"""## Dot products

Given two vectors $\mathbf{u}$ and $\mathbf{v}$, the dot product $\mathbf{u}^T \mathbf{v}$ is a sum over the products of the corresponding elements: $\mathbf{u}^T \mathbf{v} = \sum_{i=1}^{d} u_i \cdot v_i$.
"""

x = nd.arange(4) +1.0
y = nd.ones(4)
print(x, y, nd.dot(x, y))

"""Note that we can express the dot product of two vectors ``nd.dot(u, v)`` equivalently by performing an element-wise multiplication and then a sum:"""

nd.sum(x * y)

"""## Matrix-vector products

$$A=\begin{pmatrix}
 a_{11} & a_{12} & \cdots & a_{1m} \\
 a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \cdots & a_{nm} \\
\end{pmatrix},\quad\mathbf{x}=\begin{pmatrix}
 x_{1}  \\
 x_{2} \\
\vdots\\
 x_{m}\\
\end{pmatrix} $$

$$A\mathbf{x}=
\begin{pmatrix}
\cdots & \mathbf{a}^T_{1} &...  \\
\cdots & \mathbf{a}^T_{2} & \cdots \\
 & \vdots &  \\
 \cdots &\mathbf{a}^T_n & \cdots \\
\end{pmatrix}
\begin{pmatrix}
 x_{1}  \\
 x_{2} \\
\vdots\\
 x_{m}\\
\end{pmatrix}
= \begin{pmatrix}
 \mathbf{a}^T_{1} \mathbf{x}  \\
 \mathbf{a}^T_{2} \mathbf{x} \\
\vdots\\
 \mathbf{a}^T_{n} \mathbf{x}\\
\end{pmatrix}
$$

So you can think of multiplication by a matrix $A\in \mathbb{R}^{m \times n}$ as a transformation that projects vectors from $\mathbb{R}^{m}$ to $\mathbb{R}^{n}$.

We can also use matrix-vector products to describe the calculations of each layer in a neural network.
Expressing matrix-vector products in code with ``ndarray``, we use the same ``nd.dot()`` function as for dot products.
"""

A = A.reshape((5,4))
print(nd.dot(A, x))
print(A * x)

"""## Matrix-matrix multiplication

If you've gotten the hang of dot products and matrix-vector multiplication, then matrix-matrix multiplications should be pretty straightforward.

Say we have two matrices, $A \in \mathbb{R}^{n \times k}$ and $B \in \mathbb{R}^{k \times m}$:

$$A=\begin{pmatrix}
 a_{11} & a_{12} & \cdots & a_{1k} \\
 a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \cdots & a_{nk} \\
\end{pmatrix},\quad
B=\begin{pmatrix}
 b_{11} & b_{12} & \cdots & b_{1m} \\
 b_{21} & b_{22} & \cdots & b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
 b_{k1} & b_{k2} & \cdots & b_{km} \\
\end{pmatrix}$$

$$AB = \begin{pmatrix}
\cdots & \mathbf{a}^T_{1} &...  \\
\cdots & \mathbf{a}^T_{2} & \cdots \\
 & \vdots &  \\
 \cdots &\mathbf{a}^T_n & \cdots \\
\end{pmatrix}
\begin{pmatrix}
\vdots & \vdots &  & \vdots \\
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
 \vdots & \vdots &  &\vdots\\
\end{pmatrix}
= \begin{pmatrix}
\mathbf{a}^T_{1} \mathbf{b}_1 & \mathbf{a}^T_{1}\mathbf{b}_2& \cdots & \mathbf{a}^T_{1} \mathbf{b}_m \\
 \mathbf{a}^T_{2}\mathbf{b}_1 & \mathbf{a}^T_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^T_{2} \mathbf{b}_m \\
 \vdots & \vdots & \ddots &\vdots\\
\mathbf{a}^T_{n} \mathbf{b}_1 & \mathbf{a}^T_{n}\mathbf{b}_2& \cdots& \mathbf{a}^T_{n} \mathbf{b}_m
\end{pmatrix}
$$

You can think of the matrix-matrix multiplication $AB$ as simply performing $m$ matrix-vector products and stitching the results together.
"""

B = nd.ones(shape=(4, 3))
nd.dot(A, B)

"""## Norms

All norms must satisfy a handful of properties:

1. $\|\alpha A\| = |\alpha| \|A\|$
1. $\|A + B\| \leq \|A\| + \|B\|$
1. $\|A\| \geq 0$
1. If $\forall {i,j}, a_{ij} = 0$, then $\|A\|=0$

To calculate the $\ell_2$ norm, we can just call ``nd.norm()``.
"""

nd.norm(x)

"""To calculate the $\ell_1$-norm we can simply perform the absolute value and then sum over the elements."""

nd.sum(nd.abs(x))
